# Epoch, training loss, validation loss
  1,0.05494,0.03929
  2,0.04104,0.03263
  3,0.03605,0.03030
  4,0.03368,0.02878
  5,0.03198,0.02771
  6,0.03053,0.02756
  7,0.03015,0.02542
  8,0.02903,0.02506
  9,0.02863,0.02488
 10,0.02832,0.02530
 11,0.02815,0.02439
 12,0.02747,0.02403
 13,0.02744,0.02358
 14,0.02713,0.02370
 15,0.02669,0.02334
 16,0.02701,0.02357
 17,0.02632,0.02316
 18,0.02621,0.02317
 19,0.02605,0.02342
 20,0.02621,0.02368
 21,0.02600,0.02503
 22,0.02559,0.02258
 23,0.02587,0.02361
 24,0.02572,0.02309
 25,0.02568,0.02237
 26,0.02565,0.02315
 27,0.02548,0.02237
 28,0.02518,0.02224
 29,0.02536,0.02280
 30,0.02502,0.02249
 31,0.02534,0.02217
 32,0.02523,0.02221
 33,0.02500,0.02243
 34,0.02485,0.02284
 35,0.02508,0.02249
 36,0.02493,0.02287
 37,0.02487,0.02256
 38,0.02486,0.02199
 39,0.02483,0.02199
 40,0.02481,0.02270
 41,0.02466,0.02187
 42,0.02458,0.02184
 43,0.02470,0.02209
 44,0.02462,0.02195
 45,0.02482,0.02170
 46,0.02445,0.02305
 47,0.02481,0.02198
 48,0.02429,0.02298
 49,0.02418,0.02166
 50,0.02458,0.02177
 51,0.02427,0.02188
 52,0.02443,0.02214
 53,0.02438,0.02182
 54,0.02401,0.02219
 55,0.02413,0.02209
 56,0.02414,0.02175
 57,0.02419,0.02156
 58,0.02427,0.02165
 59,0.02431,0.02184
 60,0.02400,0.02233
 61,0.02404,0.02202
 62,0.02416,0.02152
 63,0.02416,0.02196
 64,0.02420,0.02149
 65,0.02393,0.02182
 66,0.02414,0.02198
 67,0.02394,0.02143
 68,0.02373,0.02136
 69,0.02416,0.02152
 70,0.02406,0.02148
 71,0.02398,0.02147
 72,0.02414,0.02136
 73,0.02397,0.02147
 74,0.02407,0.02157
 75,0.02369,0.02156
 76,0.02386,0.02159
 77,0.02390,0.02143
 78,0.02330,0.02146
 79,0.02301,0.02097
 80,0.02309,0.02127
 81,0.02318,0.02105
 82,0.02316,0.02102
 83,0.02320,0.02119
 84,0.02290,0.02129
 85,0.02306,0.02122
